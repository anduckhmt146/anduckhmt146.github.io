---
layout: post
title: AI Engineering - RAG, AI Agents, Context Engineer, MCP, Fine-tuning (Weight Optimization, LoRA)
date: 2025-09-01
categories: cloud-solution
---

Here is all concepts about AI Engineering: RAG, AI Agents, Context Engineer, MCP, Fine-tuning (Weight Optimization, LoRA).

# 1. Transformer, Fine-tunning

## AI Engineer

- Transformer: predict the next token in a sequence.
- Before LLMs, each AI system do 1 task ‚áí 1 Translation, 1 Summarizer, 1 Classifier.
- Now 1 LLM can do multiple domains.

## What make LLM large

- Number of parameters: ch·ªâ s·ªë ƒë·ªÉ adjust tham s·ªë ƒë·ªÉ predict k·∫øt qu·∫£ c·ªßa model.
- Amount of data.
- Compute used to train this.

## Transformer Architecture

1. Transformer
    - Identify each words to related to each others.
2. Tokenization
    - Split the docs into multiple words.
3. Transformer Layers
    - Mask the words and self-learn.
4. Positional Encoding
    - Give the model the order of the token ‚áí generate by order.
5. Parameters
    - Patterns m√† c√°i model learn ƒë∆∞·ª£c.
6. Distributed training setup
    - Use to train data in multiple GPUs.

## How to train LLM

- Stage 0: randomly initialized model
    - H·ªèi th·∫≥ng kh√¥ng train.
- Stage 1: Pre-training
    - B·ªè data v√¥ train.
    - Cons: n√≥ ch·ªâ bi·∫øt continue the question theo data ch·ª© kh√¥ng ph√π h·ª£p v·ªõi conversation.
- Stage 2: Instruction fine-tuning
    - H∆∞·ªõng d·∫´n c√°ch tr·∫£ l·ªùi
    - Pros: gi√∫p n√≥ bi·∫øt c√°ch response theo human behavior: summary, answering.
- Stage 3: Preference fine-tuning
    - Priority c√°c response.
    - Pros: apply RLHF (reinforcement learning with human feedback) ‚áí learn ƒë·ªÉ ƒë∆∞a response match v·ªõi human.
- Stage 4: Reasoning fine-tuning
    - ƒê·∫£m b·∫£o c√≥ reason khi ƒë∆∞a ra k·∫øt qu·∫£.
    - Pros: case n√†y kh√¥ng c·∫ßn c·∫£m x√∫c c·ªßa human, c·∫ßn ch√≠nh x√°c ‚áí ƒë√∫ng th√¨ cho rewards.
1. Probabilistic thinking
    - Predict the next words based on probability.
    - Sampling instead of the selecting the highest
        - T cao, g·∫ßn nhau nhi·ªÅu th√¨ random
        - T th·∫•p th√¨ l·∫•y c√°i g·∫ßn nh·∫•t.

## LLM Generation Parameters

1. Max tokens
    - Too low: short output
    - Too long: waste compute time.
2. Temperature
    - Higher Temperature: boost creativity.
    - Low temperature: makes the model deterministic.
3. Top-k:
    - Only consider top k most likely next tokens during sampling.
    - Use case: recommend app.
4. Top-p
    - Only consider the tokens with a 90% probability are considered.
    - Use case: Q & A for high accuracy app.
5. Frequency penalty
    - D√πng trong summary.
    - Positive nghƒ©a l√† l·∫∑p l·∫°i, negative th√¨ nghƒ©a l√† ch∆∞a g·∫∑p l·∫°i.
    - Use case: summary text.
6. Presence penalty
    - Encourage the model to bring new tokens that have not been seen in the text.
    - Use case: exploration generation ideas.
7. Stop sequence:
    - 1 s·ªë k√Ω t·ª± ƒë·∫∑c bi·ªát ƒë·ªÉ model stop generate.
    - V√≠ d·ª• d·∫•u JSON ‚áí kh√¥ng split over text.
    - Use case: gen ra JSON.
8. min-p sampling
    - T√¨m min-p b·∫±ng vi·ªác ƒë·∫ßu ti√™n cho 90% tr∆∞·ªõc.
    - √çt h∆°n th√¨ d·ªùi xu·ªëng top-2, top-3, top-4,‚Ä¶

## LLM Text Generation Strategies

1. Approach 1: Greedy strategy
    - Ch·ªçn word c√≥ x√°c su·∫•t cao nh·∫•t.
    - D·∫´n ƒë·∫øn c√°c c√¢u b·ªã l·∫∑p l·∫°i.
2. Approach 2: Multinomial sampling strategy
    - Pick top token ƒë·ªÉ generate multiple sampling strategy.
    - D√πng ch·ªâ s·ªë temperature cho tr∆∞·ªùng h·ª£p n√†y.
3. Approach 3: Beam search
    - ƒêo√°n tr∆∞·ªõc next response c·ªßa ng∆∞·ªùi d√πng.
    - ƒê·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi h·ª£p l√Ω nh·∫•t.
    - Second-order thinking ƒë√∫ng nghƒ©a
4. Approach 4: Contrastive search
    - T√¨m c√°c c√¢u tr·∫£ l·ªùi m√† c√°c token kh√°c nhau.
    - ƒêa d·∫°ng g√≥c nh√¨n.
5. SLED architect - Self-logits evolution decoding  & Transformer
    - Thay v√¨ c·ªông d·ªìn t·∫•t c·∫£ t√≠nh to√°n sang layer cu·ªëi.
    - N√≥ d√πng th√¥ng tin t·ª´ c√°c layer trung gian.

## Train LLM Model

- LLM model learn from another LLM.
    - LLama 4 Scount and Maverick ‚áí learn from LLama 4 Behemoth.
    - Gemma 2 and 3 were trained using Gemini.
- Pre-training
    - Train the bigger and smaller model together.
    - LLama 4 did it.
- Post-training
    - Train the bigger model first, train smaller model later
    - Deepseek did it for Qwen and Llama 3.1

### 9.1. Soft-label distillation

- Using teacher LLM to create softmax probabilities for data.
- Use this data to train for student LLM.
- Th·∫±ng l·ªõn n√≥ process nhanh h∆°n, xong l·∫•y ƒë√≥ b·ªè cho th·∫±ng nh·ªè.
- V√≠ d·ª• c√≥ 5 tri·ªáu t·ª∑ tokens data ‚áí train ra 500 tri·ªáu GB memory (float8 precision) ‚áí l·∫•y ƒë√≥ b·ªè v√¥ th·∫±ng nh·ªè.

‚áí Softmax ki·ªÉu ƒë·ªëi v·ªõi token A, next token B x√°c su·∫•t l√† bao nhi√™u.

Question: N·∫øu d√πng 1 con LLM l·ªõn h∆°n train cho 1 con LLM nh·ªè h∆°n, c√≥ l√†m ph√¨nh size con LLM nh·ªè h∆°n ngang LLM l·ªõn h∆°n kh√¥ng ?

- Kh√¥ng
- Do tham s·ªë c≈©ng y v·∫≠y, m·ªói c√°i h√†nh vi + data l√† m·ªõi h∆°n th√¥i.
- LLM nh·ªè h·ªçc c√°ch process theo tham s·ªë ƒë√≥.
- Knowledge n·∫±m trong weights c·ªßa pattern.
- L√¥i m·∫•y c√°i pattern c·ªßa LLM l·ªõn v√†o LLM nh·ªè.

Question: So is the teacher pattern transferred?

- No pattern is transferred.
- Only constraints are transferred.
- Token probabilities (logits)
    - Output sequences
    - Preferences between alternatives
    - Reasoning traces (if you expose them)

Question: Parameter kh√¥ng ph·∫£i l√† pattern, nh∆∞ng c√†ng c√≥ nhi·ªÅu parameter th√¨ s·∫Ω c√†ng bi·ªÉu di·ªÖn ƒë∆∞·ª£c pattern.

Note:

- ‚Äú70B c√≥ th·ªÉ bi·ªÉu di·ªÖn nh·ªØng pattern m√† 7B kh√¥ng th·ªÉ‚Äù
- V√≠ d·ª•:
    - Long-chain reasoning
    - Multi-level abstraction
    - Cross-domain transfer
    - Tool planning nhi·ªÅu b∆∞·ªõc

### 9.2. Hard-label distillation

- Label do teacher giƒÉng ra.
- Thay v√¨ h·ªçc theo x√°c su·∫•t, n√†y h·ªçc theo hard label.
- DeepSeek did it to Deepseek-R1 into Qwen and Llama 3.1.
- Soft-label distillation
    - Paris: 0.85
    - Lyon: 0.1
    - Marseille: 0.05
- Hard-label distillation
    - The capital of France is Paris.

### 9.3. Co-distillation

- Hard-label: data th·∫≠t.
- Soft-label: data d·ª± ƒëo√°n ra.
- Train **2+ models song song**, m·ªói model v·ª´a:
    - h·ªçc t·ª´ **hard labels** (data th·∫≠t)
    - v·ª´a h·ªçc t·ª´ **soft predictions c·ªßa c√°c model c√≤n l·∫°i**
- M·ªôt ki·ªÉu peer-learning.

## How to run LLMs locally

1. Reason
    - Privacy data.
    - Testing things before moving to the cloud and more.
2. Ollama
    - Use like Docker to run model locally.
    - Or use Ollama to pull the model when deployment.
    - Gi·ªëng c√°i m√°y ·∫£o th√¥i.
3. LMStudio
    - D√πng ƒë·ªÉ test model local nh∆∞ chat.
    - Gi·ªëng ki·ªÉu chatGPT-like interface.
4. vLLM
    - 1 th∆∞ vi·ªán ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi vLLM interface trong code.
    - Run t·ªëi ƒëa GPU.
5. LlamaCPP
    - Run b·∫±ng CPU c·ªßa m√°y t√≠nh local.

## Transformer and Mixture of Experts

1. Transformer
    - Nh√¢n h·∫øt ma tr·∫≠n ƒë·∫øn layer cu·ªëi c√πng t·ªïng h·ª£p.
2. Mixture of experts
    - C√≥ nhi·ªÅu tham s·ªë nh∆∞ng s·ª≠ d·ª•ng small subset of tham s·ªë ƒë·ªÉ bi·ªÉu di·ªÖn pattern c·∫ßn d√πng theo chuy√™n m√¥n th√¥i.
3. Compare
    - Transformer uses a feed-forward network.
    - MoE uses experts, which are feed-forward networks but smaller compared to that in Transformer.
    
    ![image.png](/images/AI-Engineer/image.png)
    
    ![image.png](/images/AI-Engineer/image%201.png)
    
    - Cons:
        - S·∫Ω c√≥ 1 s·ªë route lu√¥n ƒëi v√†o 1 expert duy nh·∫•t.
        - C√°c expert c√≤n l·∫°i b·ªã under-trained.
        - Do n√≥ train theo c√°i h·ªèi c·ªßa ng∆∞·ªùi d√πng n·ªØa.
        - Con expert gi·ªèi c√†ng ng√†y c√†ng l√™n.
    - Solve
        - C√≥ limit t·ªëi ƒëa s·ªë tokens the expert can process.
        - Some of experts used to process the tokens.
    - Question: Notes
        - Experts kh√¥ng pre-defined t·ª´ ƒë·∫ßu.
        - M√† b·ªô router route t·ª´ t·ª´ ‚áí ƒë·ªÉ m·ªói con expert chuy√™n c√°c tham s·ªë x·ª≠ l√Ω n√≥.
        - Khi x·ª≠ l√Ω qu√° nhi·ªÅu token
            - Expert ƒë√≥ s·∫Ω b·ªã overload.
            - N√≥ horizontal ra nhi·ªÅu expert cho syntax / generic
        - Qustion: M·ªói expert params kh√°c nhau cho 1 case prompt kh√°c nhau
            
            
            | Token type | Expert c√≥ th·ªÉ d√πng |
            | --- | --- |
            | "Write" | syntax / generic |
            | "Python" | programming-ish |
            | "parse" | logic |
            | "JSON" | structured data |
        - C√†ng nhi·ªÅu tham s·ªë th√¨ c√†ng bi·ªÉu di·ªÖn ƒë∆∞·ª£c nhi·ªÅu patterns.
    - Question: T·∫°i sao MoE ra ƒë·ªùi thay cho Transformer ?
        - Observation th·ª±c t·∫ø:
            - 80% token:
                - syntax
                - common patterns
                - low entropy
            - 20% token:
                - reasoning
                - rare knowledge
                - edge cases
    - Question: C√°c params define by feature selection t·ª´ ƒë·∫ßu hay qua learned from data ?
        - KH√îNG c√≥ feature selection th·ªß c√¥ng cho expert.
        - To√†n b·ªô params c·ªßa expert ƒë∆∞·ª£c h·ªçc *end-to-end t·ª´ data*.
        - Ch·ªâ c√≥ Machine Learning l√† Feature Selection th√¥i.
    - Question: dimension x1, x2 v√† params ?
        - x1, x2: l√† s·ªë chi·ªÅu vector
        - params l√† weight cho dimension
        - Pattern: grammar, tense
        - üëâ¬†**Dimension = 4096**
        - üëâ¬†**Params = 134 tri·ªáu**
        - T·∫°i 1 th·ªùi ƒëi·ªÉm, vector bi·ªÉu di·ªÖn c√≥ 4096 *dimensions* nh∆∞ng s·ªë params tham gia t·∫°o ra vector ƒë√≥ c√≥ th·ªÉ l√† H√ÄNG TRI·ªÜU.
        - Trong kh√¥ng gian 4096D:
            - M·ªói pattern ‚âà m·ªôt vector h∆∞·ªõng
            - C√°c pattern **kh√¥ng c·∫ßn orthogonal**
            - Ch√∫ng c√≥ th·ªÉ **ch·ªìng l√™n nhau**
    - Question: FFN (**Feedforward Neural Network)** architecture
        - Gi·∫£ s·ª≠ 1 FFN layer ti√™u chu·∫©n:
            - `d_model = 4096`
            - `d_hidden = 16384`
        - **`h1`** = the **intermediate hidden activation** inside the FFN
        - **`y`** = the **output representation** of the FFN (same size as input)
            
            ```python
            h1 = GeLU(W1 ¬∑ z_l + b1)
            y  = W2 ¬∑ h1 + b2
            ```
            
        - Hidden state:
            - Do model ban ƒë·∫ßu quy ƒë·ªãnh.
            - C√†ng nhi·ªÅu data th√¨ hidden state n√†y ·ªïn h∆°n.

## Prompt Engineering

### 1. Chain Of Thought (CoT)

- Think ki·ªÉu chain of thought s·∫Ω reasoning t·ª´ng b∆∞·ªõc.
- Kh√¥ng x·∫£y ra biases.

![image.png](/images/AI-Engineer/image%202.png)

### 2. Self-Consistency (a.k.a. Majority Voting over CoT)

- V·ªõi c√πng 1 c√¢u h·ªèi, n√≥ gen nhi·ªÅu c√¢u tr·∫£ l·ªùi kh√°c nhau.
- L√† do ch·ªâ s·ªë temperature cao.
- Multiple reasoning paths v√† ch·ªçn theo s·ªë ƒë√¥ng.

![image.png](/images/AI-Engineer/image%203.png)

### 3. Tree of Thoughts (ToT)

- M·ªói b∆∞·ªõc gen N options ra, xong ch·ªçn c√°i optimize nh·∫•t.
- Kh√¥ng ch·ªët ƒë∆∞·ª£c option th√¨ l·∫•y theo c√°i ƒë√¥ng nh·∫•t ho·∫∑c th·ª±c t·∫ø nh·∫•t.
    
    ![image.png](/images/AI-Engineer/image%204.png)
    

### 4. ARQ (Attentive Reasoning Queries)

- N·∫øu kh√¥ng cho suy nghƒ© t·ª± do ‚áí s·∫Ω d·∫´n ƒë·∫øn Hallucianting.
- Force n√≥ theo rules
    - Quy ƒë·ªãnh c√°c step for reasoning.
    
    ![image.png](/images/AI-Engineer/image%205.png)
    
- ARQ - 90.2%
- CoT reasoning - 86.1%
- Direct response generation - 81.5%

![image.png](/images/AI-Engineer/image%206.png)

## Verbalized Sampling

1. 2 version of model
    - The original model: learned the rich possibilities during pre-training.
    - The safety-focused: typically bias, tr·∫£ l·ªùi c√°c c√¢u h·ªèi n√≥ gi·ªëng v·ªõi ng∆∞·ªùi d√πng.
2. C√°ch d√πng
    - ‚ÄúGenerate 5 responses with their corresponding possibilities‚Äù
    - N√≥ s·∫Ω ƒëi ƒë√†o h·∫øt logic ra l√†m.

## JSON prompting for LLMs

1. Using JSON prompt and natural processing prompt
    - Text prompt: can lead to hallucinations.
    - JSON prompts: force to consistent output.
    
    ![image.png](/images/AI-Engineer/image%207.png)
    
2. Structure means certainty
    - JSON forces you to think in terms of fields and values, which is a gift.
    - It eliminates gray areas and guesswork.
    
    ![image.png](/images/AI-Engineer/image%208.png)
    
3. You control the outputs
    - Structure the generated output.
        
        ![image.png](/images/AI-Engineer/image%209.png)
        
4. Turn the output to JSON for APIs, Databases and Apps.
    - Return JSON for BE APIs.

## BERT and GPT-3

### üîπ **BERT**

- **T√™n ƒë·∫ßy ƒë·ªß**: *Bidirectional Encoder Representations from Transformers*
- **Ki·∫øn tr√∫c**: Transformer **Encoder**
- **C√°ch h·ªçc**: *Masked Language Modeling* (che t·ª´ r·ªìi ƒëo√°n l·∫°i)
- **Hi·ªÉu ng·ªØ c·∫£nh**: **Hai chi·ªÅu** (tr√°i ‚Üî ph·∫£i)
- **M·ª•c ti√™u ch√≠nh**: Hi·ªÉu vƒÉn b·∫£n

**Gi·ªèi nh·∫•t khi l√†m**:

- Ph√¢n lo·∫°i vƒÉn b·∫£n
- Ph√¢n t√≠ch c·∫£m x√∫c
- Question Answering
- Search / ranking / NLU

---

### üîπ **GPT-3**

- **T√™n ƒë·∫ßy ƒë·ªß**: *Generative Pre-trained Transformer 3*
- **Ki·∫øn tr√∫c**: Transformer **Decoder**
- **C√°ch h·ªçc**: *Autoregressive* (ƒëo√°n token k·∫ø ti·∫øp)
- **Hi·ªÉu ng·ªØ c·∫£nh**: **M·ªôt chi·ªÅu** (tr√°i ‚Üí ph·∫£i)
- **M·ª•c ti√™u ch√≠nh**: Sinh vƒÉn b·∫£n

**Gi·ªèi nh·∫•t khi l√†m**:

- Chatbot
- Vi·∫øt b√†i / code / email
- H·ªèi ƒë√°p m·ªü
- S√°ng t·∫°o n·ªôi dung

| Ti√™u ch√≠ | **BERT** | **GPT-3** |
| --- | --- | --- |
| Ki·∫øn tr√∫c | Encoder | Decoder |
| H∆∞·ªõng ng·ªØ c·∫£nh | 2 chi·ªÅu | 1 chi·ªÅu |
| Hu·∫•n luy·ªán | Masked LM | Next-token prediction |
| Sinh text d√†i | ‚ùå Kh√¥ng | ‚úÖ R·∫•t t·ªët |
| Hi·ªÉu √Ω nghƒ©a | ‚úÖ R·∫•t m·∫°nh | ‚ö†Ô∏è T·ªët nh∆∞ng ph·ª• thu·ªôc prompt |
| Fine-tune | D·ªÖ | √çt fine-tune (prompt-based) |
| Quy m√¥ | ~110M‚Äì340M params | ~175B params |

## Fine-tunning

![image.png](/images/AI-Engineer/image%2010.png)

- GPT-3, which has 175B parameters.
- That' s 350GB of memory just to store model weights (float16 precision).
- Size of parameters ‚áí size of memory to load models.
    - If 10 users fine-tuned GPT-3 ‚Üí 3500 GB to store weights.
    - If 1000 users fine-tuned GPT-3 ‚Üí 350k GB to store weights.
    - If 100k users fine-tuned GPT-3 ‚Üí 35M GB to store weights.

![image.png](/images/AI-Engineer/image%2011.png)

## Fine-tunning Method

LoRA idea: don‚Äôt update W, add a low-rank correction

```python
input-> A -> bottleneck -> B -> useful update
```

1. LoRA
    - Th√™m low-rank matrices v√†o attention
    - Train ~0.1‚Äì2% params
    - Hi·ªáu qu·∫£ & r·∫ª
2. LLMs for user A and user B
    
    ```python
    Weffective(u) = Wbase + ŒîWu
    ```
    
- For User A
    
    ```python
    load base weights W
    load adapter ŒîW_A   ‚Üê user A only
    ```
    
- For User B
    
    ```python
    load base weights W
    load adapter ŒîW_B   ‚Üê user B only
    ```
    
1. Methods for LoRA training
    - LoRA
    - LoRA-FA
    - VeRA
    - Delta-LoRA.
    - LoRA+
    - Bonus: LoRA-drop
    - QLoRA
    - DoRA
2. When to use LoRA, when to RAG ?
    - **Need the model to *know facts* that change often?** ‚Üí **RAG**
    - **Need the model to *behave differently* (style, rules, format)?** ‚Üí **LoRA**
3. LLMs stored token embedding and weights in file.
4. Implement LoRA from Scratch
    
    ![image.png](/images/AI-Engineer/image%2012.png)
    
    ### üß† Matrix **A**
    
    - Selects **which directions** of the input space matter
    - Acts like a **feature extractor**
    - Compresses input into a small subspace
    
    ### üß† Matrix **B**
    
    - Decides **how strongly** to modify the output
    - Re-expands compressed features
    - Controls impact on model behavior

## Fine-tunning using third LLMs

1. Generate response from LLM 1, LLM2
2. LLM3 judge and rating for each response.
3. Choose the right response.

## SFT and RFT

1. SFT Process
    - It starts with a static labeled dataset of prompt‚Äìcompletion pairs.
    - Adjust the model weights to match these completions.
    - The best model (LoRA checkpoint) is then deployed for inference.
    - Supervised-learning: b·∫Øt h·ªçc theo c√°i ƒë√∫ng ƒë√≥, ki·ªÉu gia tr∆∞·ªùng nh√©t ch·ªØ k√™u n√≥ copy ƒëi.
2. RFT Process
    - RFT uses an online ‚Äúreward‚Äù approach - no static labels required.
    - The model explores diÔ¨Äerent outputs, and a Reward Function scores their
    correctness.
    - Over time, the model learns to generate higher-reward answers using
    GRPO.
    - Reward-system: c√≥ th·ªÉ 1 th·∫±ng human ƒëi judge xem c√°i n√†o l√† ƒë√∫ng.
3. About data
    - If you have data right with fact, use SFT.
    - If data ch∆∞a ƒë∆∞·ª£c label, RFT g·ªçi qua 1 con th·ª© ba ƒë·ªÉ check ƒë·ªô ch√≠nh x√°c v√† score.

## Build a Reasoning LLM using GRPO

### 1. GRPO

- Group Relative Policy Optimization: d·∫°y 1 model h·ªçc to√°n theo ki·ªÉu reinforce learning with rewards system.

### 2. Architect

![image.png](/images/AI-Engineer/image%2013.png)

- Use reward-system: update c√°i tr·ªçng s·ªë c·ªßa model.
- Use GRPO: calculate loss and update the weight of model, like fine-tuning real-time.

### 3. Load the model

- We start by loading Qwen3-4B-Base and its tokenizer using Unsloth.
- You can use any other open-weight LLM here.
    
    ![image.png](/images/AI-Engineer/image%2014.png)
    

### 4. Define LoRA config

- We 'll use LoRA to avoid fine-tuning the entire model weights. In this code, we use Unsloth' s PEFT by specifying:
    - The Model
    - LoRA low-rank
    - Modules for fine-tuning

![image.png](/images/AI-Engineer/image%2015.png)

### 5. Create the dataset

![image.png](/images/AI-Engineer/image%2016.png)

Each sample includes:

- A system prompt enforcing structured reasoning
- A question from the dataset
- The answer in the required format

### 6. Define reward functions

![image.png](/images/AI-Engineer/image%2017.png)

- Match format exactly
- Match format approximately
- Check the answer
- Check numbers

### 7. Use GRPO and start training

![image.png](/images/AI-Engineer/image%2018.png)

### 8. Comparison

![image.png](/images/AI-Engineer/image%2019.png)

### 9. The key idea of fine-tunning is calculate delta W + W_frozen

- We do not need to manual this it code.
- We only add the logic.
    
    ```jsx
    loss.backward()   ‚Üí computes ‚àáW
    optimizer.step()  ‚Üí applies ŒîW to W
    ```
    

## OpenENV: environments for Agentic RL Training

![image.png](/images/AI-Engineer/image%2020.png)

## Agent Reinforcement Trainer (ART)

![image.png](/images/AI-Engineer/image%2021.png)

- Using data l√†m chu·∫©n.
- Using 1 con Agent cao h∆°n l√†m chu·∫©n.
- Using human l√†m chu·∫©n.

---

# 2. RAG

- Prompt Engineering - which steers the model at inference time
- Fine-tuning - which adjusts its internal parameters.
- RAG: update new data as new token embeddings to the model.

![image.png](/images/AI-Engineer/image%2022.png)

## Vector Databases

- Store token embedding of the documents.
- B·∫£n ch·∫•t c≈©ng l√† compare 2 c√°i vector gi·ªØa prompt input v√† data output.

## Workflow of a RAG system

![image.png](/images/AI-Engineer/image%2023.png)

## 1. Create chunks

- The first step is to break down this additional knowledge into chunks before
embedding and storing it in the vector database.
    
    ![image.png](/images/AI-Engineer/image%2024.png)
    

## 2. Generate embeddings

![image.png](/images/AI-Engineer/image%2025.png)

## 3. Store embeddings in a vector database

![image.png](/images/AI-Engineer/image%2026.png)

## 4.  User input query and embed the query

![image.png](/images/AI-Engineer/image%2027.png)

## 5.  Retrieve similar chunks

![image.png](/images/AI-Engineer/image%2028.png)

## 6.  Re-rank the chunks

![image.png](/images/AI-Engineer/image%2029.png)

## 7.  Generate the final response

![image.png](/images/AI-Engineer/image%2030.png)

## What actually happens inside RAG

### 1Ô∏è‚É£ Retrieval (Search phase)

- User query ‚Üí embedding vector
- Compare with chunk embeddings in a **vector database**
- Retrieve top-k chunks by similarity (cosine / dot product)

```python
Query: "How does LoRA reduce trainable parameters?"
‚Üí Retrieve:
- Chunk A: Low-rank decomposition idea
- Chunk B: A¬∑B matrix update
- Chunk C: Parameter efficiency comparison
```

### 2Ô∏è‚É£ Augmentation (Prompt construction)

### 3Ô∏è‚É£ Generation (The important part)

Now the LLM:

- Reads the chunks
- Builds an internal representation (hidden states)
- **Synthesizes an answer**
- Generates **new tokens**

It may:

- Combine multiple chunks
- Infer missing steps
- Rephrase concepts
- Apply reasoning

üìå **The answer may contain words not present in any chunk**

## 5 chunking strategies for RAG

### 5.1. Fixed-size chunking

- C·∫Øt theo 1 fixed-size c·ªë ƒë·ªãnh.
    
    ![image.png](/images/AI-Engineer/image%2031.png)
    

### 5.2. Semantic chunking

![image.png](/images/AI-Engineer/image%2032.png)

![image.png](/images/AI-Engineer/image%2033.png)

### 5.3. Recursive chunking

- C·∫Øt theo k√Ω t·ª± ƒë·∫∑c bi·ªát nh∆∞ seperators or sections.
    
    ![image.png](/images/AI-Engineer/image%2034.png)
    

### 5.4. Document structure-based chunking

![image.png](/images/AI-Engineer/image%2035.png)

![image.png](/images/AI-Engineer/image%2036.png)

### 5.5. LLM-based chunking

- LLM Split the sentences to multiple chunks.

![image.png](/images/AI-Engineer/image%2037.png)

## Prompting vs. RAG vs. Finetuning

![image.png](/images/AI-Engineer/image%2038.png)

Two important parameters guide this decision:

- The amount of external knowledge required for your task.
- The amount of adaptation you need. Adaptation, in this case, means changing the behavior of the model, its vocabulary, writing style, etc.

So here's the simple takeaway:

- Use RAGs to generate outputs based on a custom knowledge base if the
vocabulary & writing style of the LLM remains the same.
- Use fine-tuning to change the structure (behaviour) of the model than
knowledge.
- Prompt engineering is suÔ¨Écient if you don't have a custom knowledge
base and don't want to change the behavior.
- And finally, if your application demands a custom knowledge base and a
change in the model's behavior, use a hybrid (RAG + Fine-tuning)
approach.

## 8 RAG architectures

### 1. Naive RAG

- Retrieves documents purely based on vector similarity between the query
embedding and stored embeddings.
- Works best for simple, fact-based queries where direct semantic matching
suÔ¨Éces.

### 2. Multimodal RAG

- Handles multiple data types (text, images, audio, etc.) by embedding and
retrieving across modalities.
- Ideal for cross-modal retrieval tasks like answering a text query with both text
and image context.

### 3. HyDE

- Hypothetical Document Embeddings
- User prompt ngu qu√° ‚áí k√™u LLM s·ª≠a prompt l·∫°i ƒë·ªÉ query d·ªÖ h∆°n.

### 4. Corrective RAG

- Compare user prompt v·ªõi th√¥ng tin t·ª´ trusted sorts tr∆∞·ªõc khi g·ª≠i cho LLM.
- ƒê·∫£m b·∫£o c√°i prompt ƒë√≥ ch·∫•t l∆∞·ª£ng.

### 5. Graph RAG

- K·∫øt qu·∫£ retrieve ra bi·∫øn n√≥ th√†nh 1 c√°i graph.
- Tr√¨nh b√†y knowledge ƒë√≥ theo d·∫°ng graph ƒë·ªÉ tr√¨nh b√†y reasoning logic h∆°n.

### 6. Hybrid RAG

- Merge nhi·ªÅu ngu·ªìn ƒë·ªÉ c√πng search.
    
    ```python
    Query
     ‚îú‚îÄ Vector Search (embeddings)
     ‚îú‚îÄ Keyword Search (BM25)
     ‚îú‚îÄ Structured DB
     ‚îî‚îÄ API / Docs
            ‚Üì
      Merge + Rerank
            ‚Üì
         LLM Answer
    ```
    

### 7. Adaptive RAG

- Break the big prompts to sub-queries for better.
- Use case: query in multiple agents.

### 8. Agentic RAG

- Use AI Agents for planning + reasoning (ReAct, CoT).
- Using memory to orchestrate retrieval
- Best for complex workflow required tool used, external apis and combine multiple RAG techniques.

## RAG vs Agentic RAG

- RAG systems may provide relevant context but don't reason through complex queries. If a query requires multiple retrieval steps, traditional RAG falls short.
- Agentic RAG is becoming increasingly popular. Let'
s understand this in more detail.
- Scan AI: Agentic RAG
- Compare

![image.png](/images/AI-Engineer/image%2039.png)

![image.png](/images/AI-Engineer/image%2040.png)

- Steps 1-2: The user inputs the query, and an agent rewrites it (removing spelling
mistakes, simplifying it for embedding, etc.)
- Step 3: Another agent decides whether it needs more details to answer the query.
- Step 4: If not, the rewritten query is sent to the LLM as a prompt.
- Step 5-8: If yes, another agent looks through the relevant sources it has access to (vector database, tools & APIs, and the internet) and decides which source should be useful. The relevant context is retrieved and sent to the LLM as a prompt.
- Step 9: Either of the above two paths produces a response.
- Step 10: A final agent checks if the answer is relevant to the query and context.
- Step 11: If yes, return the response.
- Step 12: If not, go back to Step 1. This procedure continues for a few iterations until the system admits it cannot answer the query.

## Traditional RAG vs HyDE

User prompt ngu qu√°: Con LLM use Query + document ‚áí rewrite the hypothesis prompt

![image.png](/images/AI-Engineer/image%2041.png)

![image.png](/images/AI-Engineer/image%2042.png)

## Full-model Fine-tuning vs. LoRA
vs. RAG

![image.png](/images/AI-Engineer/image%2043.png)

### 1. Full fine-tunning

- T·ªën chi ph√≠ cao v√¨ ph·∫£i fine-tune h·∫øt to√†n b·ªô c√°c weights.

### 2. LoRA fine-tunning

- Th√™m c√°c node LoRA v√†o current weights.
- Delta W, but not all the weights metrics.

### 3. RAG

- ƒêi search keyword v√† th√¥ng tin
- Xong 1 con LLM kh√°c ƒëi tr·∫£ l·ªùi.

## RAG vs REFRAG

It typically works, but at a huge cost:

- Most chunks contain irrelevant text.
- The LLM has to process far more tokens.
- You pay for compute, latency, and context.

REFRAG Method:

- Chunk compression: Each chunk is encoded into a single compressed embedding, rather than hundreds of token embeddings.
- Relevance policy: A lightweight RL-trained policy evaluates the compressed embeddings and keeps only the most relevant chunks.
- Selective expansion: Only the chunks chosen by the RL policy are expanded back into their full embeddings and passed to the LLM

Use cases:

- In some case, most of the tokens is irrelevant.
- Only need to keep the important tokens.

## RAG vs CAG

1. CAG:
    - It lets the model ‚Äúremember‚Äù stable information by caching it directly in the model‚Äôs key-value memory.
2. RAG + CAG
    - In a regular RAG setup, your query goes to the vector database, retrieves relevant chunks, and feeds them to the LLM.
    - But in RAG + CAG, you divide your knowledge into two layers.
        - The static, rarely changing data, like company policies or reference
        guides, gets cached once inside the model‚Äô
        s KV memory.
        - The dynamic, frequently updated data, like recent customer interactions or live documents, continues to be fetched via retrieval.

## RAG, Agentic RAG and AI Memory

1. RAG (2020-2023):
    - Retrieve info once, generate response
    - No decision-making, just fetch and answer
    - Problem: Often retrieves irrelevant context
2. Agentic RAG:
    - Agent decides *if* retrieval is needed
    - Agent picks *which* source to query
    - Agent validates *if* results are useful
    - Problem: Still read-only, can‚Äôt learn from interactions.
3. AI Memory:
    - Reads AND writes to external knowledge
    - Learns from past conversations
    - Use case: personalize for user context.

---

# 3. Context Engineering

## Context Engineering

- N√¢ng cao kh·∫£ nƒÉng good retrieval context cho LLMs.
- RAG workflow is typically 80% retrieval and 20% generation.
- Good retrievals are key for optimize the right result.
- Context engineer is working to enhance
    - The right data
    - The right tools.
    - The right format.
- These are the 4 key components of a context engineering system
    - Dynamic information flow: multiple data sources.
    - Smart tool access: allow agents to make actions.
    - Memory management:
        - Short-term: summary long conversation, by context window.
        - Long-term memory: user preferences.
    - Format optimization: ƒë∆∞a ra 1 response c√≥ structure than massive JSON blob.

## Context Engineering for Agents

![image.png](/images/AI-Engineer/image%2044.png)

- Instructions
- Examples
- Knowledge
- Memory
- Tools
- Guardrails

![image.png](/images/AI-Engineer/image%2045.png)

- If LLM is CPU, context window is the RAM.

## Read/Write Context for AI Agents

![image.png](/images/AI-Engineer/image%2046.png)

### 3.1. Writing context

- Long-term memory: persists across multiple sessions.
- Short-term memory: in-session.
- A state object: store the current state of multiple agents and step.

### 3.2. Read context

- A tool.
- Memory
- Knowledge base: docs, vector DB.

### 3.3. Compressing context

- Keep only tokens needed for a task.
- Preprocessing the user prompt for duplicate tokens.

### 3.4. Isolating context

- M·ªói agents ƒë·ªçc 1 context kh√°c nhau.
- D√πng state ƒë·ªÉ qu·∫£n l√Ω c√°i n√†y.

## 6 Types of Contexts for AI Agents

![image.png](/images/AI-Engineer/image%2047.png)

### 1. Instructions

- Who‚Äôs the agent ?
    - PM, Researcher, Coding Assitant.
- Why it is acting ?
    - Goal, Motivation, Outcome.
- How should it behave ?
    - Steps, tone, format, contraints.

### 2. Examples

- Model learn patterns better than plain rules.
- Example for demos and responses.

### 3. Knowledge

- External knowledge: business data, internet.
- Internal knowledge: task context.

### 4. Memory

- Short-term: current reason steps, chat history.
- Long-term: facts, company knowledge, user preferences.

### 5. Tools

- Each tool have parameters, input, examples.
- Use to call external APIs.

### 6. Tool Results

- This layer feeds the tool‚Äôs results back to the model to enable self-correction, adaptation and dynamic decision-making.

## Build a Context Engineering workflow

![image.png](/images/AI-Engineer/image%2048.png)

- User submits query.
- Fetch context from docs, web, arxiv API, and memory.
- Pass the aggregated context to an agent for filtering.
- Pass the filtered context to another agent to generate a response.
- Save the final response to memory.

Tech stack:

- Tensorlake to get RAG-ready data from complex docs
- Zep for memory
- Firecrawl for web search
- Milvus for vector DB
- CrewAI for orchestration

### 1. Crew flow

- We'll follow a top-down approach to understand the code.
- Here's an outline of what our flow looks like:
    
    ![image.png](/images/AI-Engineer/image%2049.png)
    

### 2. Prepare data for RAG

- We use Tensorlake to convert the document into RAG-ready markdown chunks for each section.
    
    ![image.png](/images/AI-Engineer/image%2050.png)
    

### 3. Indexing and retrieval

- Store chunks in vector DB.
- Query from user embedding to chunks in database.
    
    ![image.png](/images/AI-Engineer/image%2051.png)
    

### 4. Build memory layer

- Implement memory agents
- Zep acts as the core memory layer of our workflow. It creates temporal knowledge graphs to organize and retrieve context for each interaction.
- We use it to store and retrieve context from chat history and user data.
    
    ![image.png](/images/AI-Engineer/image%2052.png)
    

### 5. Firecrawl web search

- Implement web search agents

![image.png](/images/AI-Engineer/image%2053.png)

### 6. ArXiv API search

- Implement arxiv_api_agent.
    
    ![image.png](/images/AI-Engineer/image%2054.png)
    

### 7. Filter irrelevant context

- Now, we pass our combined context to the context evaluation agent that filters out irrelevant context.
- This filtered context is then passed to the synthesizer agent that generates the final response.
    
    ![image.png](/images/AI-Engineer/image%2055.png)
    

### 8. Kick oÔ¨Ä the workflow

![image.png](/images/AI-Engineer/image%2056.png)

## Claude Skills - D√πng docs tr·ªã thi√™n h·∫°

- Docs: [https://github.com/anthropics/skills/tree/main/skills](https://github.com/anthropics/skills/tree/main/skills)
- Claude Skills are Anthropic‚Äôs mechanism for giving agents reusable, persistent abilities without overloading the model‚Äôs context window.
- Because Agents forget everything so need to store context in 3 layers.
    - Layer 1: Main context.
    - Layer 2: Load by demand
    - Layer 3: Active skills when needed.
    
    ![image.png](/images/AI-Engineer/image%2057.png)
    
- The creation process is straightforward:
    1. Identify a workflow you repeat constantly.
    2. Create a skill folder and add a [skill.md](http://skill.md/) file.
    3. Write the YAML front matter + full markdown instructions.
    4. Add any scripts, examples, or supporting resources.
    5. Zip the folder and upload it in Claude‚Äôs capabilities

## Manual RAG Pipeline vs Agentic Context Engineering

### Ingestion layer:

- Connect to apps without auth headaches.
- Process diÔ¨Äerent data sources properly before embedding (email vs code vs calendar).
- Detect if a source is updated and refresh embeddings (ideally, without a full refresh).

### Retrieval layer

- Expand vague queries to infer what users actually want.
- Direct queries to the correct data sources.
- Layer multiple search strategies like semantic-based, keyword-based, and graph-based.
- Ensure retrieving only what users are authorized to see.
- Weigh old vs. new retrieved info (recent data matters more, but old context still counts).

### Generation layer

- Provide a citation-backed LLM response.

### Sample RAG System

Docs: [https://github.com/airweave-ai/airweave](https://github.com/airweave-ai/airweave)

![image.png](/images/AI-Engineer/image%2058.png)

---

# 4. AI Agents

## What is an AI Agent?

- A Research Agent autonomously searches and retrieves relevant AI research papers from arXiv, Semantic Scholar, or Google Scholar.
    
    ![image.png](/images/AI-Engineer/image%2059.png)
    
- A Filtering Agent scans the retrieved papers, identifying the most relevant ones based on citation count, publication date, and keywords.
    
    ![image.png](/images/AI-Engineer/image%2060.png)
    
- A Summarization Agent extracts key insights and condenses them into an easy-to-read report.
    
    ![image.png](/images/AI-Engineer/image%2061.png)
    
- A Formatting Agent structures the final report, ensuring it follows a clear, professional layout.
    
    ![image.png](/images/AI-Engineer/image%2062.png)
    
- Definition: Agent is decision-making system with the brain (LLM), tools (API calls), memory (context)
    
    ![image.png](/images/AI-Engineer/image%2063.png)
    

## Agent vs LLM vs RAG

- LLM is the brain.
- RAG is feeding that brain with fresh information.
- An agent is the decision-maker that plans and acts using the brain and the tools.

### 1. LLM (Large Language Model)

- It can reason, generate, summary
- But for data it already know.

### 2. RAG (Retrieval-Augmented Generation)

- Aware of knowledge update and feed into LLM.

### 3. Agent

- Agent used LLM, calls tools, RAG ‚áí make decisions and orchestrates workflows.
- Work as decision-making engine.

## Building blocks of AI Agents

### 1. Role-playing

- The way agents reasoning and retrieval process.
    
    ![image.png](/images/AI-Engineer/image%2064.png)
    

### 2. Focus/Tasks

- For example, a marketing agent should stick to messaging, tone, and audience not pricing or market analysis.
- Instead of trying to make one agent do everything, a better approach is to use multiple agents, each with a specific and narrow focus.
    
    ![image.png](/images/AI-Engineer/image%2065.png)
    

### 3. Tools

- Agents get smarter when they can use the right tools.
- For example, an AI research agent could benefit from:
    - A web search tool for retrieving recent publications.
    - A summarization model for condensing long research papers.
    - A citation manager to properly format references.
    
    ![image.png](/images/AI-Engineer/image%2066.png)
    

### 3.1.  Custom tools

**Library**: CrewAI support tools.

Tools allow the Agent to:

- Search the web for real-time data.
- Retrieve structured information from APIs and databases.
- Execute code to perform calculations or data transformations.
- Analyze images, PDFs, and documents beyond just text inputs.

![image.png](/images/AI-Engineer/image%2067.png)

### 3.2.  Custom tools via MCP

- Library: mcp-tools

![image.png](/images/AI-Engineer/image%2068.png)

### 4. Cooperation

- Instead of one agent doing everything, a team of specialized agents more focus.
- Tech lead: can split tasks and improve each other‚Äôs outputs.
    
    ![image.png](/images/AI-Engineer/image%2069.png)
    
- Consider an AI-powered financial analysis system:
    - One agent gathers data
    - another assesses risk,
    - a third builds strategy,
    - and a fourth writes the report

### 5. Guardrails

ƒê·∫∑t rule cho agents ƒë·ªÉ ƒë·∫£m b·∫£o n√≥ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng t·ª´ng step.

Examples of useful guardrails include:

- Limiting tool usage: Prevent an agent from overusing APIs or generating irrelevant queries.
- Setting validation checkpoints: Ensure outputs meet predefined criteria before moving to the next step.
- Establishing fallback mechanisms: If an agent fails to complete a task, another agent or human reviewer can intervene.

![image.png](/images/AI-Engineer/image%2070.png)

### 6. Memory

D√πng ƒë·ªÉ improve ch·∫•t l∆∞·ª£ng agents sau c√°c l·∫ßn d√πng.

![image.png](/images/AI-Engineer/image%2071.png)

DiÔ¨Äerent types of memory in AI agents include:

- Short-term memory: during execution or session or previous question.
- Long-term memory: persists after execution or multiple interactions.
- Entity memory: store about the key subjects discussed, about tracking customer details in CRM Agents ‚áí Knowledge graph entities.

## Memory Types in AI Agents

Based on scope

- Short-term
- Long-term

Based on human, long-term memory in agents can be:

- Semantic: facts and knowledge.
- Episodic: chuy·ªán qu√° kh·ª©.
- Procedural: quy tr√¨nh ƒë·ªÉ suy nghƒ©

![image.png](/images/AI-Engineer/image%2072.png)

## Importance of Memory for Agentic Systems

![image.png](/images/AI-Engineer/image%2073.png)

N·∫øu kh√¥ng c√≥ tr√≠ nh·ªõ:

- N√≥ s·∫Ω kh√¥ng nh·ªõ c√°c th√¥ng tin c·ªßa b·∫°n.
- V√≠ d·ª• c√¢u h·ªèi: ‚ÄúM√†u th√≠ch y√™u th√≠ch nh·∫•t c·ªßa t·ªõ l√† g√¨ ?‚Äù

![image.png](/images/AI-Engineer/image%2074.png)

C√≥ 5 lo·∫°i tr√≠ nh·ªõ c·ªßa Agents

- Short-term memory
- Long-term memory
- Entity memory
- Contextual memory: facts and knowledge
- User memory

N√≥ d√πng 1 ph·∫ßn memory c·ªßa n√≥ ƒë·ªÉ x·ª≠ l√Ω th√¥ng tin

![image.png](/images/AI-Engineer/image%2075.png)

## 5 Agentic AI Design Patterns

![image.png](/images/AI-Engineer/image%2076.png)

### 1. Reflection Pattern

- 1 con LLM generate, 1 con LLM spot mistakes
- Cho ƒë·∫øn khi n√†o final response ƒë∆∞·ª£c.

![image.png](/images/AI-Engineer/image%2077.png)

### 2. Tool use pattern

- Tools allow LLMs to gather more information by:
    - Querying a vector database
    - Executing Python scripts
    - Invoking APIs, etc.
- M·ªói call LLM call 1 tool kh√°c nhau.

![image.png](/images/AI-Engineer/image%2078.png)

### 3. ReAct (Reason and Act) pattern

- Reflection thinking + action ‚áí adjust the prompt.
- Merge of 2 patterns
    - Reflect the thought.
    - Interact with the world using tools.
    - Feedback loops.
    
    ![image.png](/images/AI-Engineer/image%2079.png)
    
    ![image.png](/images/AI-Engineer/image%2080.png)
    
- Thay v√¨ Thought - Thought - Thought nh∆∞ b√™n d∆∞·ªõi
    
    ![image.png](/images/AI-Engineer/image%2081.png)
    
- For example, an agent in CrewAI typically alternates between reasoning about a task and acting (using a tool) to gather information or execute steps, following the ReAct paradigm.

### 4. Planning pattern

![image.png](/images/AI-Engineer/image%2082.png)

Instead of solving a task in one go, the AI creates a roadmap by:

- Subdividing tasks
- Outlining objectives

### 5. Multi-Agent pattern

![image.png](/images/AI-Engineer/image%2083.png)

- There are several agents, each with a specific role and task.
- Each agent can also access tools.

## ReAct Implementation from Scratch

### 1. Role definition

Cung c·∫•p message ƒë·∫ßu ti√™n ƒë·ªÉ ƒë·ªãnh nghƒ©a role: system cho LLM.

![image.png](/images/AI-Engineer/image%2084.png)

This method does three things in one call:

1. Records the user input.
2. Gets the model‚Äôs reply.
3. Updates the message history for future turns.

![image.png](/images/AI-Engineer/image%2085.png)

### 2. ReAct Thinking

![image.png](/images/AI-Engineer/image%2086.png)

With this sample trace:

- The agent knows how to think.
- The agent knows how to act.
- The agent knows when to stop.

### 3. Idea

Agent **kh√¥ng ch·ªâ tr·∫£ l·ªùi**, m√† s·∫Ω:

1. **Suy nghƒ© (Reason / Thought)**: ph√¢n t√≠ch v·∫•n ƒë·ªÅ
2. **H√†nh ƒë·ªông (Act)**: g·ªçi tool / API / function
3. **Quan s√°t (Observation)**: nh·∫≠n k·∫øt qu·∫£
4. **L·∫∑p l·∫°i** cho ƒë·∫øn khi ra c√¢u tr·∫£ l·ªùi cu·ªëi

### 4. Example

### V√≠ d·ª• 1: ReAct ƒë∆°n gi·∫£n (pseudo)

**Task**: *‚ÄúTh·ªùi ti·∫øt h√¥m nay ·ªü H√† N·ªôi th·∫ø n√†o?‚Äù*

```python
Thought: T√¥i c·∫ßn bi·∫øt th·ªùi ti·∫øt hi·ªán t·∫°i ‚Üí ph·∫£i g·ªçi weather API
Action: get_weather(city="H√† N·ªôi")
Observation: 30¬∞C, n·∫Øng nh·∫π
Thought: ƒê√£ c√≥ d·ªØ li·ªáu
Final Answer: H√¥m nay H√† N·ªôi kho·∫£ng 30¬∞C, tr·ªùi n·∫Øng nh·∫π.
```

### V√≠ d·ª• 2: Cursor Application

| ReAct concept | Cursor vibe coding |
| --- | --- |
| **Thought** | Internal reasoning (hidden) |
| **Action** | Edit code, create file, refactor |
| **Tool** | File system, linter, test, grep |
| **Observation** | Compiler error, test fail, diff |
| **Loop** | Auto-iterate cho ƒë·∫øn khi OK |

## 5 Levels of Agentic AI Systems

![image.png](/images/AI-Engineer/image%2087.png)

### 5.1. Basic responder

- H·ªèi th·∫≥ng model v√† response.

### 5.2. Router pattern

- 1 con router LLM define xem c√°i tool n√†o ƒë∆∞·ª£c g·ªçi.
- R·ªìi route t·ªõi con agent ƒë√≥

### 5.3. Tool calling

- Define c·∫ßn call tool n√†o

### 5.4. Multi-agent pattern

- M·ªôt con manager tool split task.
- Xong optimize output and performance c·ªßa t·ª´ng con agent nh·ªè.

### 5.5. Autonomous pattern

- 2 con aent ƒëem ra feedback v√† validate nhau coi n√†o t·ªët h∆°n.

## 4 Layers of Agentic AI

![image.png](/images/AI-Engineer/image%2088.png)

About Agentic Infrastructure:

- Observability & logging: tracking performance and outputs (using frameworks like DeepEval).
- Error handling & retries: resilience against failures.
- Security & access control: ensuring agents don‚Äôt overstep.
- Rate limiting & cost management: controlling resource usage.
- Workflow automation: integrating agents into broader pipelines.
- Human-in-the-loop controls: allowing human oversight and intervention.

## 7 Patterns in Multi-Agent Systems

![image.png](/images/AI-Engineer/image%2089.png)

### 7.1. Parallel

- Task is executed independently, like data extraction, web retrieval, and summarization, and their outputs merge into a single result.
- Use case: Perfect for reducing latency in high-throughput pipelines like document parsing or API orchestration.

### 7.2. Sequential

- Needs step-by-step process
- Use case: workflow automation, ETL chains, and multi-step reasoning pipelines.

### 7.3. Loop

- Agents continuously refine their own outputs until a desired quality is reached.
- Use case: proofreading, report generation, or creative iteration.

### 7.4. Router

- For instance, user queries about finance go to a FinAgent, legal queries to a LawAgent.
- Use case: D√πng cho m·∫•y ki·∫øn tr√∫c MoE, hay chuy√™n m√¥n ho√° agents.

### 7.5. Aggregator

- Gom input t·ª´ nhi·ªÅu ngu·ªìn
- Use case: t·ªïng h·ª£p feedback, voting systems.

### 7.6. Network

- No clear hierarchy here, agents just talk to each other freely.
- Use case: stimulation, multi-agent games, free-form behavior, discussions.

### 7.7. Hierarchical

- M·ªôt con manager ƒë·ª©ng ra make decision.
- ƒê·∫£m b·∫£o c√°c vi·ªác nhau:
    - No two agents duplicate work.
    - Every agent knows when to act and when to wait.
    - The system collectively feels smarter than any individual part.

## MCP & A2A Protocol

- MCP l√† ƒë·ªÉ g·ªçi tool
- A2A l√† ƒë·ªÉ 2 con agent g·ªçi nhau.

![image.png](/images/AI-Engineer/image%2090.png)

![image.png](/images/AI-Engineer/image%2091.png)

## Agent-User Interaction Protocol (AG-UI)

Frontend g·ªçi tr·ª±c ti·∫øp agents lu√¥n.

![image.png](/images/AI-Engineer/image%2092.png)

Each event has an explicit payload (like keys in a Python dictionary) like:

- TEXT_MESSAGE_CONTENT for token streaming.
- TOOL_CALL_START: d√πng ƒë·ªÉ show c√°c tools ƒëang c√≥.
- STATE_DELTA: update shared state (code, data)
- AGENT_HANDOFF: pass control gi·ªØa c√°c agents.

C√°c tools li√™n quan:

- LangGraph: v·∫Ω workflow
- CrewAI: cung c·∫•p c√°c MCP.
- Mastra: define c√°i workflow cho agent trong Frontend.
- GPT-4 and Llama-3: LLM nh∆∞ m·ªôt b·ªô n√£o.

## Agent Protocol Landscape

![image.png](/images/AI-Engineer/image%2093.png)

Ho·∫∑c g·ªçi qua 1 REST qua 1 backend server c≈©ng ƒë∆∞·ª£c.

## CopilotKit

D√πng ƒë·ªÉ qu·∫£n l√Ω c√°c con agents.

![image.png](/images/AI-Engineer/image%2094.png)

## Prompt Optimization - Opik

D√πng ƒë·ªÉ optimize prompt d·ª±a tr√™n dataset.

![image.png](/images/AI-Engineer/image%2095.png)

![image.png](/images/AI-Engineer/image%2096.png)

![image.png](/images/AI-Engineer/image%2097.png)

---

## AI Agent Deployment Strategies

### 1. Batch deployment

![image.png](/images/AI-Engineer/image%2098.png)

- The Agent runs periodically, like a scheduled CLI job.
- Just like any other Agent, it can connect to external context (databases, APIs, or tools), process data in bulk, and store results.
- This typically optimizes for throughput over latency.

### 2. Stream deployment

- It continuously processes data as it flows through systems.
- Your agent stays active, handling concurrent streams while accessing both streaming storage and backend services as needed.

![image.png](/images/AI-Engineer/image%2099.png)

### 3. Real-Time deployment

- Load balancer agents for real-time processing.
- Agent as a service in micro-services.

![image.png](/images/AI-Engineer/image%20100.png)

### 4. Edge deployment

- The agent run in mobile device, laptop device.

![image.png](/images/AI-Engineer/image%20101.png)

To summarize:

- Batch = Maximum throughput
- Stream = Continuous processing
- Real-Time = Instant interaction
- Edge = Privacy + oÔ¨Ñine capability

---

# 5. MCP

Model context protocol (MCP) is a standardized interface and framework that allows AI models to seamlessly interact with external tools, resources, and environments.

![image.png](/images/AI-Engineer/image%20102.png)

### Why we need MCP

![image.png](/images/AI-Engineer/image%20103.png)

### MCP Architecture Overview

![image.png](/images/AI-Engineer/image%20104.png)

1. Host
    - User application
2. Client
    - MCP client in the host.
3. Server
    
    ![image.png](/images/AI-Engineer/image%20105.png)
    
    - MCP Server can access resources
        - Tool: executable actions
            
            ![image.png](/images/AI-Engineer/image%20106.png)
            
            ![image.png](/images/AI-Engineer/image%20107.png)
            
        - Resources: access database, file
            
            ![image.png](/images/AI-Engineer/image%20108.png)
            
        - Prompt: injected to model that server can supply.
            
            ![image.png](/images/AI-Engineer/image%20109.png)
            

### API versus MCP

- MCP used to AI Agents interact with tools.
- Centralize to build tools to interact for AI Agents.

### MCP versus Function calling

- Functional calling
    - Developers create functions with clear input and output parameters.
    - The LLM interprets the user's input to identify the appropriate function to call.
    - The application executes the identified function, processes the result, and returns the response to the user.
- MCP oÔ¨Äers a standardized protocol for integrating LLMs with external tools and data sources.

### 6 Core MCP Primitives (Hay)

![image.png](/images/AI-Engineer/image%20110.png)

### Use case

- MCP cho ph√©p t∆∞∆°ng t√°c client v·ªõi server khi t∆∞∆°ng t√°c v·ªõi tool.
- Use case: Cursor ask m cho n√≥ access file n√†o, ƒë√≥ l√† MCP Client ƒë√≥.

### MCP Client: t∆∞∆°ng t√°c v·ªõi ng∆∞·ªùi d√πng

1. Sampling
    - T·∫°o ra 2 - 3 option r·ªìi ch·ªçn ƒëi
2. Roots
    - Ch·ªçn file n√†o m cho n√≥ access.
3. Elicitations
    - H·ªèi th√™m th√¥ng tin t·ª´ ng∆∞·ªùi d√πng.

### MCP Server: th·ª±c hi·ªán c√°c action

1. Tools
    - Action n√≥ ƒë∆∞·ª£c l√†m
2. Resources
    - Trigger v√†o resources n√†o.
3. Prompt
    - Guide the LLM how to use tools and resources.

Docs: [https://github.com/mcp-use/mcp-use](https://github.com/mcp-use/mcp-use)

### Creating MCP Agents

![image.png](/images/AI-Engineer/image%20111.png)

### Common Pitfall: Tool Overload in MCP Client

### 1. Tool-name hallucinations

- The model may invent a tool that does not exist. This usually happens when the tool list is large or poorly named.

### 2. Confusion between similar tools

- If a server exposes several tools with overlapping responsibilities, the model may struggle to choose the correct one.

### 3. Degraded decision quality with large toolsets

- Presenting too many tools at once increases cognitive load for the LLM, leading to inconsistent tool selection or unnecessary calls

## Solution: The Server Manager

![image.png](/images/AI-Engineer/image%20112.png)

- Load tools from specific server when needed.

## Creating MCP Client

![image.png](/images/AI-Engineer/image%20113.png)

![image.png](/images/AI-Engineer/image%20114.png)

## Creating MCP Server

![image.png](/images/AI-Engineer/image%20115.png)

### Resources

Resources expose read-only content such as files or generated text through a stable URI.

![image.png](/images/AI-Engineer/image%20116.png)

### Prompts

Prompts define reusable instruction templates that agents can invoke to generate structured messages.

![image.png](/images/AI-Engineer/image%20117.png)

### Sampling

Sampling lets your server ask the client‚Äôs model to generate text mid-workflow.

![image.png](/images/AI-Engineer/image%20118.png)

### Elicitation

Elicitation requests structured input from the user, such as selecting an option or entering text.

![image.png](/images/AI-Engineer/image%20119.png)

### Notifications

Notifications allow your server to push asynchronous updates such as progress or status changes to the client.

![image.png](/images/AI-Engineer/image%20120.png)

### MCP Inspector

![image.png](/images/AI-Engineer/image%20121.png)

- Browse and test tools interactively
- Explore resources and inspect their content
- Preview prompts and validate arguments
- Watch sampling and notification events in real time
- Monitor all JSON-RPC traÔ¨Éc between client and server

### MCP UI

![image.png](/images/AI-Engineer/image%20122.png)

![image.png](/images/AI-Engineer/image%20123.png)

### Deploy MCP Server

![image.png](/images/AI-Engineer/image%20124.png)

---

# 6. LLM Optimization

## Why do we need optimization?

![image.png](/images/AI-Engineer/image%20125.png)

![image.png](/images/AI-Engineer/image%20126.png)

- Model A is more accurate, but it is significantly slower and much larger.
- Model B is slightly less accurate but is faster, smaller, and far easier to deploy.

## Model Compression

Goal: They aim to make the model smaller - that is why the name ‚Äúmodel compression‚Äù.

![image.png](/images/AI-Engineer/image%20127.png)

### 1. Knowledge Distillation

- Idea: d√πng 1 con LLM teacher tr√™n train cho 1 con LLM student.

![image.png](/images/AI-Engineer/image%20128.png)

### 2. Pruning

![image.png](/images/AI-Engineer/image%20129.png)

### 3. Low-rank Factorization

- Gi·∫£m chi·ªÅu c√°i matrix khi nh√¢n nhi·ªÅu matrix.
- M·ªói c√°i node l√† 1 matrix nh·ªè h∆°n.

![image.png](/images/AI-Engineer/image%20130.png)

### 4. Quantization

ƒê·ªïi t·ª´ 16 bit sang 8 bit, 4 bit.

![image.png](/images/AI-Engineer/image%20131.png)

## Regular ML Inference vs. LLM Inference

Inference: D·ª± ƒëo√°n c√°i ·∫£nh l√† g√¨ t·ª´ d·ªØ li·ªáu h·ªçc ƒë∆∞·ª£c.

### Continuous batching

- Traditional models, like CNNs, have a fixed-size image input and a fixed-length output (like a label).
    
    ![image.png](/images/AI-Engineer/image%20132.png)
    
- LLMs, however, deal with variable-length inputs (the prompt) and generate variable-length outputs.
    
    ![image.png](/images/AI-Engineer/image%20133.png)
    

This keeps the GPU pipeline full and maximizes utilization.

![image.png](/images/AI-Engineer/image%20134.png)

### CPU vs GPU (core idea)

### üß† CPU

- Few powerful cores (usually 4‚Äì32)
- Optimized for:
    - Logic
    - Branching
    - Sequential tasks
- Great at **doing one thing very well**

### üöÄ GPU

- Thousands of small cores
- Optimized for:
    - **Massively parallel math**
    - Same operation on lots of data
- Great at **doing many simple things at once**

### KV Caching in LLMs

Note: 

- KV: Key-value caching.

![image.png](/images/AI-Engineer/image%20135.png)

---

# 7. LLM Evaluation

## G-Eval

- G-Eval is a task-agnostic LLM as a Judge metric in Opik that solves this.
- ƒê√°nh gi√° output c·ªßa 1 con LLM.

![image.png](/images/AI-Engineer/image%20136.png)

![image.png](/images/AI-Engineer/image%20137.png)

## LLM Arena-as-a-Judge

![image.png](/images/AI-Engineer/image%20138.png)

1 con LLM ƒë·ª©ng ra judge 2 output:

- Create an ArenaTestCase with a list of ‚Äúcontestants‚Äù
and their respective LLM interactions.
- Next, define your criteria for comparison using the Arena G-Eval metric, which incorporates the G-Eval algorithm for a comparison use case.
- Finally, run the evaluation and print the scores.

## Multi-turn Evals for LLM Apps

Test nhi·ªÅu role v·ªã tr√≠ kh√°c nhau xem tr·∫£ l·ªùi nh∆∞ th·∫ø n√†o.

![image.png](/images/AI-Engineer/image%20139.png)

## Evaluating MCP-powered LLM apps

There are primarily 2 factors that determine how well an MCP app works:

- If the model is selecting the right toolEvaluating MCP-powered LLM apps?
- And if it's correctly preparing the tool call?

Vi·∫øt testcase cho test MCP.

![image.png](/images/AI-Engineer/image%20140.png)

## Component-level Evals for LLM Apps

![image.png](/images/AI-Engineer/image%20141.png)

- Tracing nhi·ªÅu b∆∞·ªõc khi call 1 model

## Red teaming LLM apps

- ƒêi ƒëo c√°i bias v√† toxic c·ªßa model
- Bias also accepts ‚ÄúGender‚Äù, ‚ÄúPolitics‚Äù, and ‚ÄúReligion‚Äù
as types.
- Toxicity accepts ‚Äúprofanity‚Äù, ‚Äúinsults‚Äù, ‚Äúthreats‚Äù  and ‚Äúmockery‚Äù as types.

![image.png](/images/AI-Engineer/image%20142.png)

![image.png](/images/AI-Engineer/image%20143.png)

---

# 8. LLM Deployment

### VLLM deployment

1. Using continuous batching
2. KV - caching method.
3. Smart Scheduling (Prefill vs Decode)
4. Prefix-Aware Routing
5. LoRA and Multi-Model Support
6. Familiar OpenAI-Compatible API

### vLLM: An LLM Inference Engine

- Underutilized GPUs: traditional batching leaves GPUs idle because requests complete at diÔ¨Äerent times
- Wasteful KV-cache memory: contiguous KV-cache storage causes fragmentation.
- DiÔ¨Écult developer experience: many high-performance systems require custom code and custom API

N√≥ h·ªó tr·ª£ m·∫•y c√°i caching model d√πm ƒë·ª° ph·∫£i l√†m

### LitServe: Custom inference engine

![image.png](/images/AI-Engineer/image%20144.png)

---

# 9. LLM Observability

### Evaluation vs Observability

![image.png](/images/AI-Engineer/image%20145.png)

![image.png](/images/AI-Engineer/image%20146.png)